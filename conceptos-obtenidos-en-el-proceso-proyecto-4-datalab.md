# üìö Conceptos Obtenidos en el Proceso Proyecto 4: DataLab

* **El rango intercuart√≠lico (IQR)** es una medida estad√≠stica que se utiliza para describir la dispersi√≥n o variabilidad en un conjunto de datos. Para calcular el IQR, primero necesitas ordenar tus datos de menor a mayor. Luego, divides tus datos en cuatro partes iguales, conocidas como cuartiles.

El IQR es la diferencia entre el tercer cuartil (Q3) y el primer cuartil (Q1). Matem√°ticamente, se expresa as√≠:

$$IQR=Q3‚àíQ1$$

Donde: $$Q1$$ es el valor que deja el 25% de los datos por debajo y el 75% por encima. $$Q3$$ es el valor que deja el 75% de los datos por debajo y el 25% por encima.

En resumen, el IQR te da una idea de la variabilidad en el 50% medio de tus datos, eliminando la influencia de los valores extremos. Es √∫til porque no se ve afectado por valores at√≠picos o extremos, proporcionando as√≠ una medida m√°s robusta de la dispersi√≥n en comparaci√≥n con el rango completo de los datos.

* **Histograma sesgado:** hacia la derecha es cuando la distribuci√≥n de datos indica que los valores altos ocurren con baja frecuencia. Este modelo tambi√©n se conoce com√∫nmente como el modelo de "cola derecha", ya que ‚Äúse afina" a medida que nos movemos a lo largo del eje x, lo que indica que la frecuencia est√° disminuyendo.
* **Histograma sim√©trico** (o unimodal) centra los datos en la media (medida central) y tiene caracter√≠sticas a trav√©s de la distribuci√≥n de la media y la desviaci√≥n est√°ndar. Una caracter√≠stica del histograma sim√©trico es contener la mayor cantidad de datos del centro del gr√°fico. En estad√≠stica, este modelo se llama normal y le permite analizar cu√°nto se desv√≠an otros datos de este modelo.
*   **Un boxplot:** tambi√©n conocido como diagrama de caja y bigotes, es una representaci√≥n gr√°fica que muestra la distribuci√≥n de un conjunto de datos. Este gr√°fico proporciona informaci√≥n sobre la mediana, los cuartiles y la presencia de posibles valores at√≠picos en los datos. El "caja" en el centro del gr√°fico representa el rango intercuart√≠lico (IQR), que es la diferencia entre el primer cuartil (Q1) y el tercer cuartil (Q3). Los "bigotes" se extienden desde la caja hasta los valores extremos, pero generalmente no m√°s all√° de 1.5 veces el IQR.

    En resumen, un boxplot es una herramienta visual que ayuda a entender la dispersi√≥n y la distribuci√≥n de un conjunto de datos, destacando aspectos clave como la mediana, los cuartiles y posibles valores at√≠picos.
* **El m√©todo Z-score:** tambi√©n conocido como puntaje Z o est√°ndar Z, es una medida estad√≠stica que cuantifica la posici√≥n de un punto de datos con respecto a la media de un conjunto de datos y expresa esa posici√≥n en t√©rminos de desviaciones est√°ndar. Es una manera de normalizar los datos y compararlos en una escala com√∫n. El resultado Z indica cu√°ntas desviaciones est√°ndar un valor particular est√° por encima o por debajo de la media. Si el Z-score es positivo, significa que el valor est√° por encima de la media, y si es negativo, est√° por debajo de la media.
*   **Riesgo Relativo:**

    El riesgo relativo es una medida que se utiliza en estudios epidemiol√≥gicos para comparar la probabilidad de que un evento ocurra en dos grupos diferentes. Por lo general, se emplea en investigaciones sobre la relaci√≥n entre un factor de riesgo (como fumar) y el desarrollo de una enfermedad. Se calcula dividiendo la tasa de incidencia del evento en el grupo expuesto al factor de riesgo entre la tasa de incidencia en el grupo no expuesto.

    La f√≥rmula es la siguiente:  **Riesgo Relativo= Riesgo de incidencia en el grupo expuesto ‚Äã/  Riesgo de incidencia en el grupo no expuesto.**                                Un riesgo relativo igual a 1 indica que no hay diferencia en el riesgo entre ambos grupos. Un riesgo relativo mayor a 1 indica un mayor riesgo en el grupo expuesto, mientras que un riesgo relativo menor a 1 indica un menor riesgo en el grupo expuesto.
*   **Incidencia:**

    La incidencia se refiere a la frecuencia con la que ocurre un evento (como una enfermedad) en una poblaci√≥n durante un per√≠odo de tiempo espec√≠fico. Se expresa como la tasa de incidencia, que es el n√∫mero de nuevos casos de la enfermedad dividido por la poblaci√≥n en riesgo durante ese per√≠odo.

    La f√≥rmula de la tasa de incidencia es: **Tasa de Incidencia=( N√∫mero de nuevos casos/Poblaci√≥n de Riesgo) ‚Äã √ó1000**                                                             La incidencia proporciona una medida de la velocidad a la que ocurren los nuevos casos y es √∫til para evaluar la carga de enfermedad en una poblaci√≥n.                                                                                                                       En resumen, el riesgo relativo compara las tasas de incidencia entre dos grupos, mientras que la incidencia en s√≠ misma mide la frecuencia de un evento en una poblaci√≥n. Ambos conceptos son fundamentales en la epidemiolog√≠a para entender la relaci√≥n entre factores de riesgo y enfermedades.
* **Univariado: Definici√≥n:** "Uni" significa uno, por lo que un an√°lisis univariado se centra en una sola variable a la vez. **Enfoque:** Examina las caracter√≠sticas o propiedades de una √∫nica variable sin considerar ninguna relaci√≥n con otras variables. **Ejemplo:** Si estudias la altura de un grupo de personas, un an√°lisis univariado se centrar√≠a solo en la variable "altura" sin tener en cuenta ninguna otra caracter√≠stica.
* **Bivariado: Definici√≥n:** "Bi" significa dos, as√≠ que un an√°lisis bivariado se enfoca en dos variables simult√°neamente. **Enfoque:** Busca entender la relaci√≥n o asociaci√≥n entre dos variables, analizando c√≥mo cambia una variable en funci√≥n de la otra. **Ejemplo:** Si estudias la relaci√≥n entre la cantidad de horas de estudio y las calificaciones obtenidas por los estudiantes, est√°s realizando un an√°lisis bivariado.
* **Multivariado: Definici√≥n:** "Multi" significa muchos, por lo que un an√°lisis multivariado implica el estudio simult√°neo de tres o m√°s variables. **Enfoque:** Examina las interacciones complejas entre m√∫ltiples variables, considerando c√≥mo varias variables se relacionan entre s√≠ de manera conjunta. **Ejemplo:** Si analizas la influencia de la cantidad de horas de estudio, el nivel de ansiedad y la calidad del sue√±o en el rendimiento acad√©mico, est√°s realizando un an√°lisis multivariado.                                                                                                    **En resumen, univariado se enfoca en una variable, bivariado en dos variables para explorar su relaci√≥n, y multivariado en tres o m√°s variables para comprender las interacciones complejas entre ellas.**
*   **Variables Dummies: Definici√≥n:** Las variables dummy son creadas para representar informaci√≥n cualitativa, como categor√≠as, grupos o caracter√≠sticas que no son num√©ricas. **Valores:** Estas variables toman solo dos valores posibles, generalmente 0 o 1. El valor 1 indica la presencia de la caracter√≠stica, mientras que el valor 0 indica su ausencia. **Ejemplo:** supongamos que estamos estudiando el impacto del tipo de combustible en el rendimiento de los autom√≥viles. Podr√≠amos crear una variable dummy llamada "Diesel" que tome el valor 1 si el autom√≥vil funciona con diesel y 0 si no. De manera similar, podr√≠amos tener otra variable dummy llamada "Gasolina" que tome el valor 1 si el autom√≥vil funciona con gasolina y 0 si no. **Uso en Regresi√≥n:** En modelos de regresi√≥n, las variables dummy se utilizan para representar variables categ√≥ricas en t√©rminos num√©ricos, permitiendo que el modelo las incluya en sus c√°lculos. **Evitar Multicolinealidad:** Cuando se tienen varias categor√≠as, es com√∫n usar una categor√≠a como referencia y crear variables dummy para las dem√°s categor√≠as. Esto ayuda a evitar problemas de multicolinealidad en el modelo.

    **En resumen, las variables dummy son una manera de representar informaci√≥n cualitativa o categ√≥rica de manera num√©rica en an√°lisis estad√≠sticos, facilitando la inclusi√≥n de estas caracter√≠sticas en modelos matem√°ticos como los de regresi√≥n.**
* **Precisi√≥n (Accuracy):** La precisi√≥n se calcula como el n√∫mero de predicciones correctas (verdaderos positivos m√°s verdaderos negativos) dividido por el total de observaciones.

**Permite visualizar cu√°ntas predicciones fueron correctas o incorrectas en t√©rminos de positivos y negativos.**

* **Accuracy (Precisi√≥n): Significado:** Representa la proporci√≥n de predicciones correctas con respecto al total de predicciones del modelo. **Ejemplo:** Un valor de 1.0 significa que todas las predicciones del modelo fueron correctas, es decir, el 100% de precisi√≥n.
* **Confusion Matrix (Matriz de Confusi√≥n): Significado:** Una tabla que muestra la cantidad de predicciones correctas e incorrectas, divididas en clases. **Ejemplo:** En una matriz de confusi√≥n, se pueden ver cu√°ntas predicciones fueron correctas para la clase 0 y la clase 1.
* **Classification Report (Informe de Clasificaci√≥n): Significado:** Proporciona m√©tricas detalladas de rendimiento del modelo, incluyendo precisi√≥n, recuperaci√≥n (sensibilidad), F1-score y soporte. **Ejemplo:** Permite evaluar no solo la precisi√≥n general, sino tambi√©n el rendimiento espec√≠fico para cada clase.
* **Precision (Precisi√≥n): Significado:** Indica la proporci√≥n de predicciones positivas correctas entre todas las predicciones positivas. **Ejemplo:** Una precisi√≥n del 100% significa que todas las predicciones positivas del modelo son correctas.
* **Recall (Recuperaci√≥n o Sensibilidad): Significado:** Muestra la proporci√≥n de instancias positivas correctamente identificadas en comparaci√≥n con el total de instancias positivas. **Ejemplo:** Un recall del 100% indica que el modelo identifica todas las instancias positivas.
* **F1-score: Significado:** Es una m√©trica que combina precisi√≥n y recuperaci√≥n en un solo n√∫mero. Es √∫til cuando hay un desequilibrio en las clases. **Ejemplo:** Un F1-score de 1.0 indica un rendimiento perfecto, mientras que valores m√°s bajos sugieren un equilibrio entre precisi√≥n y recuperaci√≥n.
* **Support (Soporte): Significado:** La cantidad de instancias de cada clase en los datos de prueba. **Ejemplo:** Puede indicar si hay un desequilibrio en la distribuci√≥n de las clases en el conjunto de datos.
* **Promedio Ponderado: Significado:** Se refiere al promedio ponderado de las m√©tricas, donde se tiene en cuenta el soporte de cada clase. **Ejemplo:** Si hay m√°s instancias de una clase, su rendimiento tendr√° m√°s peso en el promedio ponderado.
*   La Regresi√≥n Lineal Ordinaria (OLS, por sus siglas en ingl√©s) es un m√©todo estad√≠stico que se utiliza para analizar la relaci√≥n entre una variable dependiente (la que queremos predecir) y una o m√°s variables independientes (las que usamos para hacer la predicci√≥n). El objetivo principal es encontrar la l√≠nea de mejor ajuste que minimiza la suma de los cuadrados de las diferencias entre los valores reales y los valores predichos.

    En t√©rminos m√°s sencillos, la regresi√≥n lineal ordinaria busca modelar la relaci√≥n lineal entre las variables. La ecuaci√≥n de la l√≠nea de regresi√≥n tiene la forma $$y=mx+b$$,  donde $$y$$ es la variable dependiente, $$x$$ es la variable independiente, $$m$$ es la pendiente de la l√≠nea y $$b$$ es la intersecci√≥n con el eje vertical.

    El m√©todo OLS busca encontrar los valores de $$m$$ y $$b$$ de manera que la suma de las diferencias al cuadrado entre los valores observados y los predichos sea m√≠nima. Esto se logra ajustando la l√≠nea de tal manera que la distancia vertical entre los puntos y la l√≠nea de regresi√≥n sea lo m√°s peque√±a posible.

    En resumen, la regresi√≥n lineal ordinaria es una herramienta estad√≠stica que nos ayuda a entender y modelar la relaci√≥n entre variables, asumiendo que esta relaci√≥n es lineal.
*   **El test de Mann-Whitney U** es una prueba no param√©trica que se utiliza para comparar dos muestras independientes y determinar si hay diferencias significativas entre ellas. Esta prueba es √∫til cuando tus datos no cumplen con los supuestos necesarios para realizar una prueba t de Student.

    Aqu√≠ tienes los pasos b√°sicos y una explicaci√≥n pr√°ctica:

    1. **Datos de entrada:**
       * Tienes dos grupos independientes.
       * Cada grupo tiene observaciones, pero no es necesario que tengan el mismo n√∫mero de datos.
    2. **Ordenar los datos:**
       * Combina los datos de ambos grupos y ord√©nalos de menor a mayor, asign√°ndoles un rango en caso de empates.
    3. **Calcular la suma de rangos:**
       * Para cada grupo, suma los rangos asignados a sus observaciones.
    4. **Calcular el estad√≠stico U:**
       * Usa la f√≥rmula para calcular el estad√≠stico U, que est√° basado en las sumas de rangos.
    5. **Comparar con el valor cr√≠tico:**
       * Consulta una tabla de valores cr√≠ticos de U o utiliza software estad√≠stico para determinar si el valor de U calculado es significativo.
    6. **Interpretar el resultado:**
       * Si el valor de U es menor que el valor cr√≠tico, se rechaza la hip√≥tesis nula, lo que sugiere que hay diferencias significativas entre los dos grupos.

    El test de Mann-Whitney U te ayuda a determinar si hay diferencias significativas entre dos grupos, sin hacer suposiciones espec√≠ficas sobre la distribuci√≥n de los datos. Es especialmente √∫til cuando trabajas con datos ordinales o cuando no puedes asumir que tus datos siguen una distribuci√≥n normal.
*   **La regresi√≥n log√≠stica** es un m√©todo estad√≠stico utilizado en an√°lisis de datos para predecir la probabilidad de que ocurra un evento binario. Este evento binario significa que solo hay dos posibles resultados, como s√≠/no, √©xito/fracaso, o 1/0.

    La regresi√≥n log√≠stica toma variables independientes (o caracter√≠sticas) y las utiliza para calcular la probabilidad de que ocurra el resultado deseado. A diferencia de la regresi√≥n lineal, que se utiliza para predecir valores num√©ricos, la regresi√≥n log√≠stica se aplica cuando la variable dependiente es categ√≥rica.

    La salida de la regresi√≥n log√≠stica se interpreta como la probabilidad de que el evento ocurra. Adem√°s, se utiliza una funci√≥n log√≠stica para transformar la combinaci√≥n lineal de las variables independientes en un valor entre 0 y 1, que puede interpretarse como una probabilidad.

    La regresi√≥n log√≠stica es una herramienta valiosa para modelar y predecir eventos binarios basados en variables explicativas.

    \
    \


\


\
